{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24064713",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "## Regression Analysis\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This lab introduces you to the Regression problems and how to apply the solutions. Furthermore, you will learn how to prepare the dataset into a machine learning model.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Data exploration\n",
    "- Data preparation\n",
    "- Implementing Linear Regression algorithm\n",
    "- Encoding Categorical Data (Converting categorical data to numbers)\n",
    "- Data Scaling\n",
    "- Implementing Polynomial Regression algorithm\n",
    "- Multiple linear regression\n",
    "\n",
    "\n",
    "### Regression\n",
    "1. What is regression?\n",
    "2. Why we need to explore the Data for Regression, specifically?\n",
    "3. What methods do we usually use for data preparation?\n",
    "3. When Do We Need Regression?\n",
    "\n",
    "\n",
    "### Linear Regression\n",
    "1. What is linear regression?\n",
    "</span>\n",
    "\n",
    "####  In this regression task we will predict the percentage of marks that a student is expected to score based upon the number of hours they studied. This is a simple linear regression task as it involves just one explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0709f23e",
   "metadata": {},
   "source": [
    "#### Import the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156980ca",
   "metadata": {},
   "source": [
    "#### Load data and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('res/student_scores.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b32131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4aae5",
   "metadata": {},
   "source": [
    "##### Data Plotting:\n",
    "Plot the data points on 2-D graph to eyeball our dataset and see if we can manually find any relationship between the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa483276",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(x='Hours', y='Scores', style='o')\n",
    "plt.title('Hours vs Percentage')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Percentage Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a53558",
   "metadata": {},
   "source": [
    "#### Splitting data into train / test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "# Write one line below to split the dataset into train / test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ae635",
   "metadata": {},
   "source": [
    "#### Build Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdfa0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# Train the model on the X_train and y_train, write one line below\n",
    "\n",
    "\n",
    "print(regressor.intercept_)\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0300d6",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the linear regression model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get predictions of the X_test by our model, assign the result into y_pre value, write one line below\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d21e20",
   "metadata": {},
   "source": [
    "Task: Measure the performance of the model using the root mean squared error, mean absolute error and coefficient of determination $R^2$.\n",
    "\n",
    "Recall the formulas of the MAE:\n",
    "\n",
    "![MAE formula](res/mae_formula.png \"MAE formula\")\n",
    "\n",
    "MSE:\n",
    "\n",
    "![MSE formula](res/mse_formula.png \"MSE formula\")\n",
    "\n",
    "MSE:\n",
    "\n",
    "![RMSE formula](res/rmse.png \"RMSE formula\")\n",
    "\n",
    "\n",
    "Q: What RMSE, MSE and R-squared tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68803bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7cbd8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encoding Categorical Data (Converting categorical data to numbers)\n",
    "\n",
    "There are two common approaches for converting ordinal and categorical variables to numerical values. They are:\n",
    "\n",
    "- Ordinal Encoding\n",
    "- One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350806e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ordinal Encoding\n",
    "\n",
    "In ordinal encoding, each unique category value is assigned an integer value.\n",
    "\n",
    "For example, “red” is 1, “green” is 2, and “blue” is 3.\n",
    "\n",
    "This is called an ordinal encoding or an integer encoding and is easily reversible. Often, integer values starting at zero are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12699b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# example of a ordinal encoding\n",
    "from numpy import asarray\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d4414",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's convert the above categorical data into numerical via Ordinal Encoder method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a032f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# todo: write one line code below to transform categorical data into numerical via OrdinalEncoder\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddcbb7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. When should we use OrdinalEncoding?\n",
    "\n",
    "2. When should not we use OrdinalEncoding? What are the disadvantages of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dc538",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "\" Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be “on.” This is called one-hot encoding … \" — Page 78, Feature Engineering for Machine Learning, 2018.\n",
    "\n",
    "We can demonstrate the usage of the OneHotEncoder on the color categories. First the categories are sorted, in this case alphabetically because they are strings, then binary variables are created for each category in turn. This means blue will be represented as [1, 0, 0] with a “1” in for the first binary variable, then green, then finally red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4818829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a one hot encoding\n",
    "from numpy import asarray\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69eaafb",
   "metadata": {},
   "source": [
    "Let's convert the above categorical data into numerical via One-Hot Encoder method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e563b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# todo: write one line code below to transform categorical data into numerical via OneHotEncoder\n",
    "\n",
    "\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662e961",
   "metadata": {},
   "source": [
    "## Data Scaling\n",
    "\n",
    "Machine learning algorithm just sees number — if there is a vast difference in the range say few ranging in thousands and few ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.\n",
    "\n",
    "The machine learning algorithm works on numbers and does not know what that number represents. A weight of 10 grams and a price of 10 dollars represents completely two different things — which is a no brainer for humans, but for a model as a feature, it treats both as same.\n",
    "\n",
    "Suppose we have two features of weight and price, as in the below table. The “Weight” cannot have a meaningful comparison with the “Price.” So the assumption algorithm makes that since “Weight” > “Price,” thus “Weight,” is more important than “Price.”\n",
    "\n",
    "![Fruit weight price example](res/fruit_weight_price.png \"Fruit example\")\n",
    "\n",
    "So these more significant number starts playing a more decisive role while training the model. Thus feature scaling is needed to bring every feature in the same footing without any upfront importance. Interestingly, if we convert the weight to “Kg,” then “Price” becomes dominant.\n",
    "\n",
    "Another reason why feature scaling is applied is that few algorithms like Neural network gradient descent converge much faster with feature scaling than without it.\n",
    "\n",
    "![Converge example](res/converge_example.png \"Converge example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742da3e",
   "metadata": {},
   "source": [
    "### Data scaling technique: Data normalization\n",
    "Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1 (or other ranges).\n",
    "\n",
    "Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data.\n",
    "\n",
    "The formula for normalization:\n",
    "\n",
    "$x^{(i)}_{\\operatorname{normalized}} =  \\frac{x^{(i)} - min(X)}{max(X) - min(X)}$, where $X = [x_0, x_1, ... x_i, ... x_n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n",
    "                   'PRICE': [1,3,2,5]},\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a2f83",
   "metadata": {},
   "source": [
    "Let's apply the min-max scaler, or normalization to the above dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7070a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# todo: apply min-max scaler to the df dataset, write one line code below\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame(min_max_scaled_data, columns=['WEIGHT','PRICE'], index = ['Orange','Apple','Banana','Grape'])\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07432b6c",
   "metadata": {},
   "source": [
    "### Data scaling technique: Data Standardization\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1. It is sometimes referred to as “whitening.”\n",
    "\n",
    "This can be thought of as subtracting the mean value or centering the data.\n",
    "\n",
    "Like normalization, standardization can be useful, and even required in some machine learning algorithms when your data has input values with differing scales.\n",
    "\n",
    "Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your data if this expectation is not met, but you may not get reliable results.\n",
    "\n",
    "Standardization requires that you know or are able to accurately estimate the mean and standard deviation of observable values. You may be able to estimate these values from your training data.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$y = \\frac{(x - mean)} {standard\\_deviation}$\n",
    "\n",
    "where:\n",
    "- $mean = \\frac{sum(x)}{count(x)}$\n",
    "- $standard\\_deviation = \\sqrt{\\frac{\\sum (x - mean)^2}{count(x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n",
    "                   'PRICE': [1,3,2,5]},\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36da915",
   "metadata": {},
   "source": [
    "Let's apply the standard scaler to the above dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# todo: apply standard scaler to the df dataset, write one line code below\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(scaled_df,\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f886b",
   "metadata": {},
   "source": [
    "Data standardization vs normalization, which one to use?\n",
    "\n",
    "\n",
    "![Feature scaling, standartization vs Max min](res/f_scaling_stand_vs_max_min.png \"Feature scaling, standartization vs Max min\")\n",
    "\n",
    "![Feature scaling](res/f_scaling_c_age.png \"Feature scaling\")\n",
    "\n",
    "![Feature scaling](res/f_scaling_c_salary.png \"Feature scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ed4e4",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Polynomial Regression, is simply a transformation for the explanatory variables to higher polynomial orders with interactive variables.\n",
    "1. Why do we need it?\n",
    "2. Which order of the polynomial should we choose? \n",
    "2. Should we always pick the most complex model? \n",
    "\n",
    "#### We will create a synthetic dataset by adding some random gaussian noise to a cosinusoidal function.\n",
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe38453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96231930",
   "metadata": {},
   "source": [
    "#### Create sinusoidal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04f037",
   "metadata": {},
   "source": [
    "#### Create a synthetic dataset by adding some random gaussian noise to a cosinusoidal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed5f34",
   "metadata": {},
   "source": [
    "#### We will build three polynomial models with degrees [1, 4, 15] and observe the effect of increasing the degree of complixity of the model on how well it suits the data.\n",
    "\n",
    "\n",
    "Task: Write a pipeline of PolynomialFeatures transformation then, LinearRegression mode to be applied on X & y.\n",
    "Then get the cross validation scores with the appropriate scoring argument with k = 10.\n",
    "\n",
    "Q: What's your observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc01ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i])\n",
    "    linear_regression = LinearRegression()\n",
    "\n",
    "    # write 2 lines of code to create a pipeline that contains\n",
    "    #   polynomial_features and linear_regression;\n",
    "    #   also fit the created pipeline on the X dataset\n",
    "    \n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    # write one line bewlow to calculate the cross validation score\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808d525",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "When should we use multiple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c932f",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "As dataset, we will use the following dataset:\n",
    "https://www.kaggle.com/mohansacharya/graduate-admissions\n",
    "\n",
    "We have already downloaded the CSV of the dataset (Admission_Predict.csv) and we just load it through pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('res/Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f78e8",
   "metadata": {},
   "source": [
    "Let's get familiar with the dataet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca36cb9",
   "metadata": {},
   "source": [
    "Now let's understand the format of the dataset, i.e. datatypes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0cd35",
   "metadata": {},
   "source": [
    "For more convenience working with dataset, let's convert it from pandas dataframe into numpy array. Also, we should select float as a dataformat of all the columns.\n",
    "\n",
    "\n",
    "\n",
    "Let's extract feature variable (X) and label variable (Y) from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cadaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0,8]\n",
    "X = df.drop(df.columns[cols],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9409c",
   "metadata": {},
   "source": [
    "Let's explore the dataset with Seaborn.\n",
    "\n",
    "First, let's see how the range of the columns' values differ from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New boxplot using stats_df\n",
    "plt.figure(figsize=(9,6)) # Set plot dimensions\n",
    "sns.boxplot(data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012b6ac",
   "metadata": {},
   "source": [
    "As you we can see, the GRE Score column and TOEFL column have high values compared to other columns. This can have a negative influence on our model, since the large values will impact on the training process more.\n",
    "\n",
    "That's why we need to apply normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f49dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "columns_ = ['GRE Score','TOEFL Score', 'University Rating','SOP','LOR','CGPA','Research']\n",
    "\n",
    "# todo: apply min-max scaler to the df dataset, write one line code below\n",
    "\n",
    "X = pd.DataFrame(X, columns=columns_)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.iloc[:, [8]]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a86df",
   "metadata": {},
   "source": [
    "Split X and Y into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# write one line code to split the dataset into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6369d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239e16c",
   "metadata": {},
   "source": [
    "Now, let's instantinate the linear regression model and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# write one line code to train the model on the train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = model.intercept_\n",
    "print(intercept)\n",
    "coefficients = model.coef_\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327586ca",
   "metadata": {},
   "source": [
    "Now let's predict the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0455f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write one line code to predect the test dataset by the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00964c95",
   "metadata": {},
   "source": [
    "Now we will create a dataframe to compare the predicted and actual labels (Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e57f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.squeeze(Y_pred)\n",
    "\n",
    "Y_test = np.squeeze(Y_test)\n",
    "\n",
    "df_ = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4081002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# write three lines of code to calculate the MAE, MSE and RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f631b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References:\n",
    "- https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/\n",
    "- https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6df0ddd77085922c773681b1c23afa6ec355a7eb5a25c833f534ec75c0111436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
